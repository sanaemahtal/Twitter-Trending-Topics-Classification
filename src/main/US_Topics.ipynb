{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "from decimal import *\n",
    "from twitter import Twitter, OAuth,TwitterHTTPError,TwitterStream\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import KFold    \n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.cross_validation import KFold    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cross_validation import train_test_split #  this is used for cross validation \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn import linear_model\n",
    "#LOGESTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# THIS IS FOR THE DISPLAY\n",
    "from numpy import loadtxt, where\n",
    "from pylab import scatter, show, legend, xlabel, ylabel\n",
    "\n",
    "# from pandas\n",
    "import pandas as pd\n",
    "\n",
    "#from matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "\n",
    "\n",
    "import json, time, sys, io, pickle,os\n",
    "import re\n",
    "import tweepy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# THIS IS FOR THE DISPLAY\n",
    "from numpy import loadtxt, where\n",
    "from pylab import scatter, show, legend, xlabel, ylabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Authorization\n",
    "consumer_key = '7lFSsRohBO5Yj4z2SkzfZq94K'\n",
    "consumer_key_s = '0IICR5iqIBgJbb5rlTVTMbFATlGXmSSXIhkNIxqdDSOcuOtz7H'\n",
    "access_token = '294692499-27bExhbYgGBSMCPVkwU9aSavnYuNpshi9W7zCcHd'\n",
    "access_token_secret = 'vXt9Ce7zW2V7JwHZ6NzIa2VS284m4ywz1CMnJOKK8iyUy'\n",
    "# OAuth process, using the keys and tokens\n",
    "oauth = OAuth(access_token, access_token_secret, consumer_key, consumer_key_s)\n",
    "twitter = Twitter(auth=oauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Get Trending Topics\n",
    "sfo_trends = twitter.trends.place(_id = 23424977)\n",
    "data = sfo_trends[0]\n",
    "trends = data['trends']\n",
    "names = [trend['name'].encode(\"utf-8\") for trend in trends]\n",
    "trendsName=[]\n",
    "trendsName.append(names)\n",
    "print(trendsName)\n",
    "\"\"\"\n",
    "\n",
    "#Manually Save in File and Read \n",
    "f = open('TrendTopics.txt','r')\n",
    "trends = f.read().split(\",\")\n",
    "#print(trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Process Tweets\\ndef processTweet(tweet):\\n    \\n    #Convert to lower case\\n    #Remove additional white spaces\\n    #Replace #word with word\\n    #trim\\n    #Replace all Punctuations\\n    tweet = tweet.lower()\\n    tweet = re.sub(\\'[\\\\s]+\\', \\' \\', tweet)\\n    tweet = re.sub(r\\'#([^\\\\s]+)\\', r\\'\\x01\\', tweet)\\n    tweet = tweet.strip(\\'\\'\"\\')\\n    tweet = re.sub(\\'[^a-zA-z ]\\',\"\",tweet)\\n    return tweet\\n \\ndef processTweetsAndSave(filename):\\n    fp = open(filename, \\'r\\')\\n    fpn = open(filename+\\'_process.txt\\',\\'w\\')\\n    line = fp.readline()\\n    while line:\\n        processedTweet = processTweet(line)\\n        print>>fpn, processedTweet\\n        line = fp.readline()\\n    fp.close()\\n\\n\\n# for i in trends:\\n#     processTweetsAndSave(\\'Data/\\'+i+\\'.txt\\')\\nfor i in trends:\\n    f = open(\\'Data/\\'+i+\\'.txt\\', \\'w\\')\\n    tweets = twitter.search.tweets(q=i+\" -filter:links\", lang=\\'en\\', count=1000)\\n    tweet_s = tweets[\\'statuses\\']\\n    for t in tweet_s:\\n        if(not t[\\'text\\'].startswith(\"RT\")):\\n            f.write(t[\\'text\\'].encode(\"utf-8\")+\"\\n\")\\n    f.close()\\n    processTweetsAndSave(\\'Data/\\'+i+\\'.txt\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Process Tweets\n",
    "def processTweet(tweet):\n",
    "    \n",
    "    #Convert to lower case\n",
    "    #Remove additional white spaces\n",
    "    #Replace #word with word\n",
    "    #trim\n",
    "    #Replace all Punctuations\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    tweet = re.sub('[^a-zA-z ]',\"\",tweet)\n",
    "    return tweet\n",
    " \n",
    "def processTweetsAndSave(filename):\n",
    "    fp = open(filename, 'r')\n",
    "    fpn = open(filename+'_process.txt','w')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        processedTweet = processTweet(line)\n",
    "        print>>fpn, processedTweet\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "# for i in trends:\n",
    "#     processTweetsAndSave('Data/'+i+'.txt')\n",
    "for i in trends:\n",
    "    f = open('Data/'+i+'.txt', 'w')\n",
    "    tweets = twitter.search.tweets(q=i+\" -filter:links\", lang='en', count=1000)\n",
    "    tweet_s = tweets['statuses']\n",
    "    for t in tweet_s:\n",
    "        if(not t['text'].startswith(\"RT\")):\n",
    "            f.write(t['text'].encode(\"utf-8\")+\"\\n\")\n",
    "    f.close()\n",
    "    processTweetsAndSave('Data/'+i+'.txt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in trends:\\n    f1 = open(\\'Data/\\'+i+\\'.txt\\'+\\'_process.txt\\',\\'r\\').readlines() \\n    f2 = open(\\'stopwordslist.txt\\',\\'r\\').readlines() \\n    f3 = open(\\'Data/\\'+i+\\'1.txt\\',\\'w\\')\\n    for line1 in f1:\\n        for line2 in f2:\\n            line2 = line2.rstrip()\\n            if line2 in line1:\\n                line1 = re.sub(r\\'\\x08\\' + line2 + r\\'\\x08\\'+\" \",\"\",line1)\\n        f3.write(line1)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in trends:\n",
    "    f1 = open('Data/'+i+'.txt'+'_process.txt','r').readlines() \n",
    "    f2 = open('stopwordslist.txt','r').readlines() \n",
    "    f3 = open('Data/'+i+'1.txt','w')\n",
    "    for line1 in f1:\n",
    "        for line2 in f2:\n",
    "            line2 = line2.rstrip()\n",
    "            if line2 in line1:\n",
    "                line1 = re.sub(r'\\b' + line2 + r'\\b'+\" \",\"\",line1)\n",
    "        f3.write(line1)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []    \n",
    "for i in trends:\n",
    "    f3 = open('DataS/'+i+'1.txt','r').readlines()\n",
    "    words =[]\n",
    "    for line in f3:\n",
    "        for ele in line.split(\" \"):\n",
    "            words.append(ele)\n",
    "    \n",
    "    filename = \"dict.txt\"\n",
    "    A=[]\n",
    "    B=[]    \n",
    "    with open(filename, \"r\") as filestream:\n",
    "        for line in filestream:\n",
    "            current = line.split(\",\")\n",
    "            A.append(current[0])\n",
    "            B.append(current[1].strip())\n",
    "    unq_words = np.unique(words)\n",
    "    count=[]\n",
    "    for i in range(len(unq_words)):\n",
    "        for j in range(len(A)):\n",
    "            if(unq_words[i]==A[j]):\n",
    "                count.append((unq_words[i],words.count(unq_words[i]),B[j]))\n",
    "    count = np.asarray(count)\n",
    "    sum = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        for j in range(len(count)):\n",
    "            if(int(count[j][2])==i):\n",
    "                sum[i]=sum[i]+int(count[j][1])\n",
    "    X.append(np.asarray(sum))\n",
    "    if(not np.max(sum)==0):\n",
    "        Y.append(sum.tolist().index(np.max(sum)))\n",
    "    else:\n",
    "        Y.append(6)\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    setData(y_actual_data,y_predicted data)Taking in the actual result and the predicted result in the \n",
    "    getMatrics(): this will get back the confussion matrics with the specificity\n",
    "    Params: \n",
    "    - y_test,\n",
    "    - y_pred_class\n",
    "    Return:\n",
    "        returns TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision\n",
    "\"\"\"\n",
    "class TestMetrics:\n",
    "    \n",
    "    def setData(self,y_test,y_pred_class):\n",
    "            self.y_test = y_test\n",
    "            self.y_pred_class = y_pred_class\n",
    "    \n",
    "    def getMatrics(self):\n",
    "        confusion = metrics.confusion_matrix(self.y_test, self.y_pred_class)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "\n",
    "        Classification_Accuracy = metrics.accuracy_score(self.y_test, self.y_pred_class)\n",
    "        Classification_Error =  1 - metrics.accuracy_score(self.y_test, self.y_pred_class)\n",
    "        Sensitivity = TP / float(TP + FN)\n",
    "        Specificity = TN / float(TN + FP) \n",
    "        False_Positive_Rate = FP / float(TN + FP)\n",
    "        Precision = TP / float(TP + FP)\n",
    "        return TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Error  FN  FP  False_Positive_Rate  Precision  Sensitivity  \\\n",
      "0       1.0    0.0   0   0                    0          1          1.0   \n",
      "1       1.0    0.0   0   0                    0          1          1.0   \n",
      "2       1.0    0.0   0   0                    0          1          1.0   \n",
      "3       1.0    0.0   0   0                    0          1          1.0   \n",
      "4       0.8    0.2   1   0                    0          1          0.5   \n",
      "\n",
      "   Specificity  TN  TP  \n",
      "0            1   1   4  \n",
      "1            1   1   3  \n",
      "2            1   1   4  \n",
      "3            1   1   1  \n",
      "4            1   3   1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "matricsTotal = []\n",
    "ld_matricsTotal = []\n",
    "kf = sklearn.cross_validation.KFold(n=len(Y), n_folds=10, shuffle=True,random_state=5)\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test =np.array([list(X[a]) for a  in train_index]),np.array([list(X[a]) for a  in test_index])\n",
    "    Y_train, Y_test = [Y[a] for a  in train_index],[Y[a] for a  in test_index]\n",
    "    #print X_train\n",
    "    \n",
    "     ### sklearn test \n",
    "    ld_grad =   LogisticRegression()\n",
    "    ld_grad.fit(X_train,Y_train)\n",
    "    ld_predicted_Y_values = ld_grad.predict(X_test)\n",
    "    \n",
    "    ld_tm  = TestMetrics()\n",
    "    ld_tm.setData(Y_test,ld_predicted_Y_values)\n",
    "    TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision =  ld_tm.getMatrics()\n",
    "    ld_matricsTotal.append({\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN,\"Accuracy\":Classification_Accuracy,\"Error\":Classification_Error,\"Sensitivity\":Sensitivity,\"Specificity\":Specificity,\"False_Positive_Rate\":False_Positive_Rate,\"Precision\":Precision})\n",
    " \n",
    "    \n",
    "#print \"sklearn\"\n",
    "ld_df = pd.DataFrame(ld_matricsTotal).head()\n",
    "ld_df.plot(kind = 'bar',figsize =(10 ,10),stacked = False)\n",
    "print(ld_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Error  FN  FP  False_Positive_Rate  Precision  Sensitivity  \\\n",
      "0       0.8    0.2   0   0                    0          1          1.0   \n",
      "1       0.6    0.4   0   0                  NaN          1          1.0   \n",
      "2       1.0    0.0   0   0                    0          1          1.0   \n",
      "3       0.4    0.6   0   0                    0          1          1.0   \n",
      "4       0.8    0.2   1   0                    0          1          0.5   \n",
      "\n",
      "   Specificity  TN  TP  \n",
      "0            1   1   3  \n",
      "1          NaN   0   2  \n",
      "2            1   1   4  \n",
      "3            1   1   1  \n",
      "4            1   3   1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "matricsTotal = []\n",
    "ld_matricsTotal = []\n",
    "kf = sklearn.cross_validation.KFold(n=len(Y), n_folds=10, shuffle=True,random_state=5)\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test =np.array([list(X[a]) for a  in train_index]),np.array([list(X[a]) for a  in test_index])\n",
    "    Y_train, Y_test = [Y[a] for a  in train_index],[Y[a] for a  in test_index]\n",
    "    #print X_train\n",
    "    \n",
    "     ### sklearn test \n",
    "    ld_grad =   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "    ld_grad.fit(X_train,Y_train)\n",
    "    ld_predicted_Y_values = ld_grad.predict(X_test)\n",
    "    \n",
    "    ld_tm  = TestMetrics()\n",
    "    ld_tm.setData(Y_test,ld_predicted_Y_values)\n",
    "    TP,TN,FP,FN,Classification_Accuracy,Classification_Error,Sensitivity,Specificity,False_Positive_Rate,Precision =  ld_tm.getMatrics()\n",
    "    ld_matricsTotal.append({\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN,\"Accuracy\":Classification_Accuracy,\"Error\":Classification_Error,\"Sensitivity\":Sensitivity,\"Specificity\":Specificity,\"False_Positive_Rate\":False_Positive_Rate,\"Precision\":Precision})\n",
    " \n",
    "    \n",
    "#print \"sklearn\"\n",
    "ld_df = pd.DataFrame(ld_matricsTotal).head()\n",
    "ld_df.plot(kind = 'bar',figsize =(10 ,10),stacked = False)\n",
    "print(ld_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
